---
permalink: /
title: "Lishuang Zhan &#124; 詹李双"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span style="font-size: 14px;">I am currently a Ph.D student (since fall, 2021) in [School of Informatics, Xiamen University](https://informatics.xmu.edu.cn/), advised by Prof. [Shihui Guo](https://www.humanplus.xyz/). My research focuses on natural human-computer interaction, flexible wearables, and multimodal sensing.</span>

# Education
<table style="width:100%; border-collapse: collapse; border: none; font-size: 14px; line-height: 0.8;">
  <tr>
    <td style="width:15%; border: none;">2021-now</td>
    <td style="width:85%; border: none;">Xiamen University, Successive Master-Doctor in Computer Science and Technology</td>
  </tr>
  <tr>
    <td style="width:15%; border: none;">2017-2021</td>
    <td style="width:85%; border: none;">Xiamen University, Bachelor in Digital Media Technology</td>
  </tr>
</table>

# News
<table style="width:100%; border-collapse: collapse; border: none; font-size: 14px; line-height: 0.8;">
  <tr>
    07/2024: Our paper [SATPose](https://openreview.net/forum?id=VnaNemZgPj) on multimodal 3D human pose estimation is accepted to **ACM MM 2024**.
  </tr>
  <tr>
    02/2024: Our paper [Loose Inertial Poser](https://ieeexplore.ieee.org/document/10657915) on motion capture with loose-wear IMUs is accepted to **CVPR 2024**.
  </tr>
</table>

- <span style="font-size: 14px;">07/2024: Our paper [SATPose](https://openreview.net/forum?id=VnaNemZgPj) on multimodal 3D human pose estimation is accepted to **ACM MM 2024**.</span>
- <span style="font-size: 14px;">02/2024: Our paper [Loose Inertial Poser](https://ieeexplore.ieee.org/document/10657915) on motion capture with loose-wear IMUs is accepted to **CVPR 2024**.</span>
- <span style="font-size: 14px;">10/2023: Our paper [TouchEditor](https://dl.acm.org/doi/abs/10.1145/3631454?af=R) on flexible text editing system is accepted to **Ubicomp/IMWUT 2024**.</span>
- <span style="font-size: 14px;">06/2023: Our paper [Touch-and-Heal](https://dl.acm.org/doi/abs/10.1145/3596258) on data-driven affective computing is accepted to **Ubicomp/IMWUT 2023**.</span>
- <span style="font-size: 14px;">01/2023: Our paper [Touchable Robot Dog](https://ieeexplore.ieee.org/document/10161049) on human-robot-dog tactile interaction is accepted to **ICRA 2023**.</span>
- <span style="font-size: 14px;">10/2022: Our paper [Handwriting Velcro](https://dl.acm.org/doi/10.1145/3569461) on flexible text input system is accepted to **Ubicomp/IMWUT 2023**.</span>
- <span style="font-size: 14px;">08/2022: Our paper [Sparse Flexible Mocap](https://dl.acm.org/doi/10.1145/3564700) on sparse joint tracking is accepted to **TOMMCCAP 2023**.</span>
- <span style="font-size: 14px;">10/2021: Our paper MSMD-VisPro on data processing and visualization won the **<span style="color: red;">Best Poster Paper</span>** on **ChinaVR 2021**.</span>

# Publications
<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/acmmm_satpose.jpg" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://openreview.net/forum?id=VnaNemZgPj">SATPose: Improving Monocular 3D Pose Estimation with Spatial-aware Ground Tactility</a></strong><br>
      <strong>Lishuang Zhan</strong>, Enting Ying, Jiabao Gan, Shihui Guo*, Boyu Gao, Yipeng Qin<br>
      <em>Proceedings of the 32nd ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2024</em><br>
      SATPose is a novel multimodal approach for 3D human pose estimation to mitigate the depth ambiguity inherent in monocular solutions by integrating spatial-aware pressure information.
    </td>
  </tr>
</table>

<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/cvpr_looseinertialposer.jpg" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://ieeexplore.ieee.org/document/10657915">Loose Inertial Poser: Motion Capture with IMU-attached Loose-Wear Jacket</a></strong><br>
      Chengxu Zuo, Yiming Wang, <strong>Lishuang Zhan</strong>, Shihui Guo*, Xinyu Yi, Feng Xu, Yipeng Qin<br>
      <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024</em><br>
      Loose Inertial Poser is a novel motion capture solution with high wearing comfortableness by integrating four Inertial Measurement Units (IMUs) into a loose-wear jacket.
    </td>
  </tr>
</table>

<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/imwut_toucheditor.jpg" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://dl.acm.org/doi/abs/10.1145/3631454?af=R">TouchEditor: Interaction Design and Evaluation of a Flexible Touchpad for Text Editing of Head-Mounted Displays in Speech-unfriendly Environments</a></strong><br>
      <strong>Lishuang Zhan</strong>, Tianyang Xiong, Hongwei Zhang, Shihui Guo*, Xiaowei Chen, Jiangtao Gong, Juncong Lin, Yipeng Qin<br>
      <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (<strong>Ubicomp/IMWUT</strong>), 2024</em><br>
      TouchEditor is a novel text editing system for HMDs based on a flexible piezoresistive film sensor, supporting cursor positioning, text selection, text retyping and editing commands (i.e., Copy, Paste, Delete, etc.).
    </td>
  </tr>
</table>

<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/imwut_touchandheal.png" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://dl.acm.org/doi/abs/10.1145/3596258">Touch-and-Heal: Data-driven Affective Computing in Tactile Interaction with Robotic Dog</a></strong><br>
      Shihui Guo*, <strong>Lishuang Zhan*</strong>, Yancheng Cao, Chen Zheng, Guyue Zhou, Jiangtao Gong<br>
      <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (<strong>Ubicomp/IMWUT</strong>), 2023</em><br>
      We propose a data-driven affective computing system based on a biomimetic quadruped robot with large-format, high-density flexible pressure sensors, which can mimic the natural tactile interaction between humans and pet dogs.
    </td>
  </tr>
</table>

<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/icra_enable.png" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://ieeexplore.ieee.org/document/10161049">Enable Natural Tactile Interaction for Robot Dog based on Large-format Distributed Flexible Pressure Sensors</a></strong><br>
      <strong>Lishuang Zhan</strong>, Yancheng Cao, Qitai Chen, Haole Guo, Jiasi Gao, Yiyue Luo, Shihui Guo, Guyue Zhou, Jiangtao Gong<br>
      <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023</em><br>
      In this paper, we design and implement a set of large-format distributed flexible pressure sensors on a robot dog to enable natural human-robot tactile interaction.
    </td>
  </tr>
</table>

<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/tom_fullbody.png" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://dl.acm.org/doi/10.1145/3564700">Full-body Human Motion Reconstruction with Sparse Joint Tracking Using Flexible Sensors</a></strong><br>
      Xiaowei Chen, Xiao Jiang, <strong>Lishuang Zhan</strong>, Shihui Guo*, Qunsheng Ruan, Guoliang Luo, Minghong Liao, Yipeng Qin<br>
      <em>ACM Transactions on Multimedia Computing, Communications and Applications (<strong>TOMMCCAP</strong>), 2023</em><br>
      In this work, we propose a novel framework to accurately predict human joint moving angles from signals of only four flexible sensors, thereby achieving human joint tracking in multi-degrees of freedom.
    </td>
  </tr>
</table>

<table style="width:100%; border-collapse: collapse; border: none; font-size: 10px;">
  <tr>
    <td style="width:35%; border: none;"><img src="/images/imwut_handwritingvelcro.png" width="100%"></td>
    <td style="width:65%; border: none;">
      <strong><a href="https://dl.acm.org/doi/10.1145/3569461">Handwriting Velcro: Endowing AR Glasses with Personalized and Posture-adaptive Text Input Using Flexible Touch Sensor</a></strong><br>
      Fengyi Fang, Hongwei Zhang, <strong>Lishuang Zhan</strong>, Shihui Guo*, Minying Zhang, Juncong Lin, Yipeng Qin, and Hongbo Fu<br>
      <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (<strong>Ubicomp/IMWUT</strong>), 2023</em><br>
      Handwriting Velcro is a novel text input solution for AR glasses based on flexible touch sensors, which can easily stick to different body parts, thus endowing AR glasses with posture-adaptive handwriting input.
    </td>
  </tr>
</table>
